{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b131de03",
   "metadata": {},
   "source": [
    "## Load Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f95d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.utils import *\n",
    "from model.gnn import *\n",
    "from model.node2vec import *\n",
    "from model.bert_embeddings import *\n",
    "\n",
    "# Set the seeds for reproducibility\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949168b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>right_citated_text</th>\n",
       "      <th>left_citated_text</th>\n",
       "      <th>source_abstract</th>\n",
       "      <th>source_author</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_title</th>\n",
       "      <th>source_venue</th>\n",
       "      <th>source_year</th>\n",
       "      <th>target_id</th>\n",
       "      <th>target_author</th>\n",
       "      <th>target_abstract</th>\n",
       "      <th>target_year</th>\n",
       "      <th>target_title</th>\n",
       "      <th>target_venue</th>\n",
       "      <th>citated_text</th>\n",
       "      <th>citated_text_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>andsyntactic parsing .Because RNNs make very f...</td>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>Deep Neural Networks (DNNs) are powerful model...</td>\n",
       "      <td>ilya sutskever;oriol vinyals;quoc v le</td>\n",
       "      <td>1409.3215v1</td>\n",
       "      <td>Sequence to Sequence Learning with Neural Netw...</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>Modeling crisp logical regularities is crucial...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Data Recombination for Neural Semantic Parsing</td>\n",
       "      <td>ACL</td>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.Because RNNs make very few domain-specific as...</td>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>Syntactic parsing is a fundamental problem in ...</td>\n",
       "      <td>oriol vinyals;lukasz kaiser;terry koo;slav pet...</td>\n",
       "      <td>1412.7449v1</td>\n",
       "      <td>Grammar as a Foreign Language</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>Modeling crisp logical regularities is crucial...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Data Recombination for Neural Semantic Parsing</td>\n",
       "      <td>ACL</td>\n",
       "      <td>We conducted additional experiments on artific...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>; in a Pointer Network,the only way to generat...</td>\n",
       "      <td>Reproducibility. All code, data, and experimen...</td>\n",
       "      <td>We introduce a new neural architecture to lear...</td>\n",
       "      <td>oriol vinyals;meire fortunato;navdeep jaitly</td>\n",
       "      <td>1506.03134v1</td>\n",
       "      <td>Pointer Networks</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1606.03622v1</td>\n",
       "      <td>robin jia;percy liang</td>\n",
       "      <td>Modeling crisp logical regularities is crucial...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Data Recombination for Neural Semantic Parsing</td>\n",
       "      <td>ACL</td>\n",
       "      <td>Reproducibility. All code, data, and experimen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>. Recently, nsur .  have shown superior perfor...</td>\n",
       "      <td>st like CWS and POS tagging, automatic prosody...</td>\n",
       "      <td>The recently introduced continuous Skip-gram m...</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>Distributed Representations of Words and Phras...</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1511.00360v1</td>\n",
       "      <td>chuang ding;lei xie;jie yan;weini zhang;yang liu</td>\n",
       "      <td>Prosody affects the naturalness and intelligib...</td>\n",
       "      <td>2015</td>\n",
       "      <td>Automatic Prosody Prediction for Chinese Speec...</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>st like CWS and POS tagging, automatic prosody...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model trained on the Google News dataset3.In a...</td>\n",
       "      <td>We begin by considering a document as the set ...</td>\n",
       "      <td>The recently introduced continuous Skip-gram m...</td>\n",
       "      <td>tomas mikolov;ilya sutskever;kai chen 0010;gre...</td>\n",
       "      <td>1310.4546v1</td>\n",
       "      <td>Distributed Representations of Words and Phras...</td>\n",
       "      <td>NIPS</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1705.10900v1</td>\n",
       "      <td>paul michel;abhilasha ravichander;shruti rijhwani</td>\n",
       "      <td>We investigate the pertinence of methods from ...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Does the Geometry of Word Embeddings Help Docu...</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>We begin by considering a document as the set ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  right_citated_text  \\\n",
       "0  andsyntactic parsing .Because RNNs make very f...   \n",
       "1  .Because RNNs make very few domain-specific as...   \n",
       "2  ; in a Pointer Network,the only way to generat...   \n",
       "3  . Recently, nsur .  have shown superior perfor...   \n",
       "4  model trained on the Google News dataset3.In a...   \n",
       "\n",
       "                                   left_citated_text  \\\n",
       "0  We conducted additional experiments on artific...   \n",
       "1  We conducted additional experiments on artific...   \n",
       "2  Reproducibility. All code, data, and experimen...   \n",
       "3  st like CWS and POS tagging, automatic prosody...   \n",
       "4  We begin by considering a document as the set ...   \n",
       "\n",
       "                                     source_abstract  \\\n",
       "0  Deep Neural Networks (DNNs) are powerful model...   \n",
       "1  Syntactic parsing is a fundamental problem in ...   \n",
       "2  We introduce a new neural architecture to lear...   \n",
       "3  The recently introduced continuous Skip-gram m...   \n",
       "4  The recently introduced continuous Skip-gram m...   \n",
       "\n",
       "                                       source_author     source_id  \\\n",
       "0             ilya sutskever;oriol vinyals;quoc v le   1409.3215v1   \n",
       "1  oriol vinyals;lukasz kaiser;terry koo;slav pet...   1412.7449v1   \n",
       "2       oriol vinyals;meire fortunato;navdeep jaitly  1506.03134v1   \n",
       "3  tomas mikolov;ilya sutskever;kai chen 0010;gre...   1310.4546v1   \n",
       "4  tomas mikolov;ilya sutskever;kai chen 0010;gre...   1310.4546v1   \n",
       "\n",
       "                                        source_title source_venue  \\\n",
       "0  Sequence to Sequence Learning with Neural Netw...         NIPS   \n",
       "1                      Grammar as a Foreign Language         NIPS   \n",
       "2                                   Pointer Networks         NIPS   \n",
       "3  Distributed Representations of Words and Phras...         NIPS   \n",
       "4  Distributed Representations of Words and Phras...         NIPS   \n",
       "\n",
       "   source_year     target_id  \\\n",
       "0       2014.0  1606.03622v1   \n",
       "1       2014.0  1606.03622v1   \n",
       "2       2015.0  1606.03622v1   \n",
       "3       2013.0  1511.00360v1   \n",
       "4       2013.0  1705.10900v1   \n",
       "\n",
       "                                       target_author  \\\n",
       "0                              robin jia;percy liang   \n",
       "1                              robin jia;percy liang   \n",
       "2                              robin jia;percy liang   \n",
       "3   chuang ding;lei xie;jie yan;weini zhang;yang liu   \n",
       "4  paul michel;abhilasha ravichander;shruti rijhwani   \n",
       "\n",
       "                                     target_abstract  target_year  \\\n",
       "0  Modeling crisp logical regularities is crucial...         2016   \n",
       "1  Modeling crisp logical regularities is crucial...         2016   \n",
       "2  Modeling crisp logical regularities is crucial...         2016   \n",
       "3  Prosody affects the naturalness and intelligib...         2015   \n",
       "4  We investigate the pertinence of methods from ...         2017   \n",
       "\n",
       "                                        target_title target_venue  \\\n",
       "0     Data Recombination for Neural Semantic Parsing          ACL   \n",
       "1     Data Recombination for Neural Semantic Parsing          ACL   \n",
       "2     Data Recombination for Neural Semantic Parsing          ACL   \n",
       "3  Automatic Prosody Prediction for Chinese Speec...        arxiv   \n",
       "4  Does the Geometry of Word Embeddings Help Docu...        arxiv   \n",
       "\n",
       "                                        citated_text  citated_text_id  \n",
       "0  We conducted additional experiments on artific...                0  \n",
       "1  We conducted additional experiments on artific...                0  \n",
       "2  Reproducibility. All code, data, and experimen...                1  \n",
       "3  st like CWS and POS tagging, automatic prosody...                2  \n",
       "4  We begin by considering a document as the set ...                3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "df = data_preprocess('data/full_context_PeerRead.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098423a7",
   "metadata": {},
   "source": [
    "## Node2Vec Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129085be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Train Loss: 0.001, Val AUC: 0.642\n",
      "Epoch: 200, Train Loss: 0.001, Val AUC: 0.644\n",
      "Epoch: 300, Train Loss: 0.001, Val AUC: 0.645\n",
      "Epoch: 400, Train Loss: 0.001, Val AUC: 0.646\n",
      "Epoch: 500, Train Loss: 0.001, Val AUC: 0.647\n",
      "Epoch: 600, Train Loss: 0.001, Val AUC: 0.648\n",
      "Epoch: 700, Train Loss: 0.001, Val AUC: 0.648\n",
      "Epoch: 800, Train Loss: 0.001, Val AUC: 0.649\n",
      "Epoch: 900, Train Loss: 0.001, Val AUC: 0.649\n",
      "Epoch: 1000, Train Loss: 0.001, Val AUC: 0.650\n",
      "Model saved to model/node2vec_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Train a Node2Vec model and save to 'model/node2vec_model.pth'. \n",
    "run_node2vec(df, 'train', 'model/node2vec_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296730b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MRR': 0.1879389495677682,\n",
       " 'MAP@5': 0.062332882273342084,\n",
       " 'MAP@10': 0.0579965099105345,\n",
       " 'MAP@30': 0.05951454702935102,\n",
       " 'MAP@50': 0.060283805284909224,\n",
       " 'MAP@80': 0.060862072781913135,\n",
       " 'Recall@5': 0.07121131709678721,\n",
       " 'Recall@10': 0.08962863466492975,\n",
       " 'Recall@30': 0.12709981918330793,\n",
       " 'Recall@50': 0.1501050697410791,\n",
       " 'Recall@80': 0.17726019423011827}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the Node2Vec model trained previously and saved at 'model/node2vec_model.pth'. \n",
    "# This step generates Node2Vec model evaluation metrics.\n",
    "node2vec_result = run_node2vec(df, 'evaluate', 'model/node2vec_model.pth')\n",
    "node2vec_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7d81c9",
   "metadata": {},
   "source": [
    "## BERT+GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb48ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processes each citated text through a BERT model to obtain embeddings\n",
    "# Note: Running `generate_embeddings(df)` is resource-intensive and time-consuming.\n",
    "# If you have pre-generated embeddings and saved them as a .pkl file, you can skip this step.\n",
    "# Uncomment the line below if you need to generate embeddings.\n",
    "# generate_embeddings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accef902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Train Loss: 0.592, Val AUC: 0.823\n",
      "Epoch: 200, Train Loss: 0.568, Val AUC: 0.832\n",
      "Epoch: 300, Train Loss: 0.550, Val AUC: 0.840\n",
      "Epoch: 400, Train Loss: 0.532, Val AUC: 0.844\n",
      "Epoch: 500, Train Loss: 0.522, Val AUC: 0.852\n",
      "Epoch: 600, Train Loss: 0.514, Val AUC: 0.858\n",
      "Epoch: 700, Train Loss: 0.503, Val AUC: 0.864\n",
      "Epoch: 800, Train Loss: 0.494, Val AUC: 0.871\n",
      "Epoch: 900, Train Loss: 0.486, Val AUC: 0.876\n",
      "Epoch: 1000, Train Loss: 0.480, Val AUC: 0.878\n",
      "Model saved to model/gnn_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Train a GNN model and save to 'model/gnn_model.pth'.\n",
    "run_gnn(df, 'train', 'model/gnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f5f4fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MRR': 0.21987128506003825,\n",
       " 'MAP@5': 0.07586393023605455,\n",
       " 'MAP@10': 0.07415244890117617,\n",
       " 'MAP@30': 0.08168203471244725,\n",
       " 'MAP@50': 0.08451303560611,\n",
       " 'MAP@80': 0.08637684553743262,\n",
       " 'Recall@5': 0.10321673127555345,\n",
       " 'Recall@10': 0.15107529303517533,\n",
       " 'Recall@30': 0.24542951936823074,\n",
       " 'Recall@50': 0.29673323168396737,\n",
       " 'Recall@80': 0.3490031253977236}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the GNN model trained previously and saved at 'model/gnn_model.pth'. \n",
    "# This step generates GNN model evaluation metrics.\n",
    "gnn_result = run_gnn(df, 'evaluate', 'model/gnn_model.pth')\n",
    "gnn_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f4065",
   "metadata": {},
   "source": [
    "## Model Comparion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03668657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               MRR     MAP@5    MAP@10    MAP@30    MAP@50    MAP@80  \\\n",
      "Model                                                                  \n",
      "node2vec  0.187939  0.062333  0.057997  0.059515  0.060284  0.060862   \n",
      "BERT+GNN  0.219871  0.075864  0.074152  0.081682  0.084513  0.086377   \n",
      "\n",
      "          Recall@5  Recall@10  Recall@30  Recall@50  Recall@80  \n",
      "Model                                                           \n",
      "node2vec  0.071211   0.089629    0.12710   0.150105   0.177260  \n",
      "BERT+GNN  0.103217   0.151075    0.24543   0.296733   0.349003  \n"
     ]
    }
   ],
   "source": [
    "# Dictionary to hold the evaluation results of different models for comparison\n",
    "models = {\n",
    "    'node2vec': node2vec_result,  # evaluation metrics for the Node2Vec model\n",
    "    'BERT+GNN': gnn_result,  # evaluation metrics for the BERT+GNN model\n",
    "}\n",
    "\n",
    "# Generate the evaluation table for model comparison\n",
    "evaluation_table = metric_evaluation_table(models)\n",
    "\n",
    "# Print the evaluation table\n",
    "print(evaluation_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rice-project",
   "language": "python",
   "name": "rice-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
